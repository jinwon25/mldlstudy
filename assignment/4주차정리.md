# 4주차 - Chapter 06 주성분 분석 & Chapter 07 딥러닝을 시작합니다

- [MLDL 스터디 4회차 실습 코드](https://colab.research.google.com/drive/1cBWIdRLw2w1D_0ovPUEH_b-ikL-5rxfU)

## 관련 추가 내용 정리

### 딥러닝에서는 일반적으로 **교차 검증** 대신 **훈련/검증 데이터 분리 방식**을 더 많이 사용함

> **데이터가 충분히 크기 때문에**<br>
→ 딥러닝에서 사용하는 데이터셋은 대체로 크기가 커서, 검증 점수가 안정적으로 나옴
>
> **교차 검증은 연산 비용이 너무 큼**<br>
→ 딥러닝 모델은 훈련하는 데 시간이 오래 걸리기 때문에, 교차 검증처럼 여러 번 학습하는 방식은 시간과 자원이 매우 많이 소모됨

---
### **이진 분류:** loss = 'binary_crossentropy'

![스크린샷](../image/screenshot12.png)

- **출력 뉴런 수:** 1개  
- **출력값 $a$:** 시그모이드 함수 결과 → 양성 클래스일 확률  
- **손실 계산 방식:**
  - 타깃이 1인 경우: `−log(a)`
  - 타깃이 0인 경우: `−log(1−a)`

### **다중 분류:** loss = 'categorical_crossentropy'

![스크린샷](../image/screenshot13.png)

- **출력 뉴런 수:** 클래스 개수만큼 존재  
- **출력값:** 소프트맥스 함수 결과 → 각 클래스별 확률 벡터  
- **정답 라벨:** 원-핫 인코딩된 벡터 (예: `[0, 1, 0, 0, ...]`)  
- **손실 계산 방식:**  
  정답 위치의 확률값에 대해 `−log(정답 클래스 확률)` 계산  
  (나머지 클래스는 원-핫 벡터에서 0이므로 무시됨)

### **그런데 왜 `sparse`?**

- 정답 라벨을 **벡터**가 아닌 **정수 인덱스**로 표현할 수 있음 (예: 0, 1, 2)
- 이 경우 손실 함수로 **`sparse_categorical_crossentropy`** 사용  
  → 자동으로 해당 인덱스 위치만 사용하여 손실 계산 (원-핫 인코딩 불필요)

---
### **사이킷런 vs 케라스 클래스 사용법 차이**

![스크린샷](../image/screenshot14.png)

| 항목       | 사이킷런                            | 케라스                                           |
|------------|-------------------------------------|--------------------------------------------------|
| 손실 함수  | `'log_loss'` (로지스틱 손실)       | `'sparse_categorical_crossentropy'`             |
| 반복 횟수  | `max_iter` 매개변수로 지정         | `epochs` 매개변수로 지정                        |
| 출력 설정  | 자동 처리                          | 출력 뉴런 수 직접 지정 (`Dense(10)`)            |
| 평가 방법  | `.score()` 함수 사용                | `.evaluate()` 함수 사용                         |

---
### **왜 은닉층에 활성화 함수를 사용해야 할까?**

신경망의 은닉층에 **활성화 함수**를 사용하는 이유는 단순히 숫자 계산만 하는 선형 연산으로는 복잡한 문제를 해결할 수 없기 때문임

![스크린샷](../image/screenshot15.png)

- 은닉층이 있어도 **결과적으로는 하나의 선형 방정식**으로 표현됨<br>
→ 즉, **은닉층의 존재 의미가 사라짐**

![스크린샷](../image/screenshot16.png)

- 중간에 비선형 함수 `log()`가 포함되면 전체 네트워크를 단순한 선형식으로 표현할 수 없음<br>
→ **복잡한 함수 학습 가능**, **모델의 표현력 증가**

---
### **렐루 함수가 효과적인 이유는?**

| 구분             | 설명                                                                 |
|------------------|----------------------------------------------------------------------|
| 계산 간편성      | 수식이 `ReLU(z) = max(0, z)` 형태로 매우 단순해 계산 비용이 적음               |
| 기울기 소실 완화 | 시그모이드와 달리 양수 영역의 기울기가 1로 일정하여, 역전파 시 그레이디언트가 잘 전달됨 |
| 희소성 유도      | 음수 입력은 0이 되어 일부 뉴런이 비활성화됨 → 모델의 일반화 성능에 기여           |
| 비선형성 유지    | 선형처럼 보이지만 0을 기준으로 비선형 함수이므로 딥러닝 모델 구성에 적합함          |

---
### **`EarlyStopping` vs `ModelCheckpoint` 비교**

| 항목                  | `EarlyStopping` 콜백                                         | `ModelCheckpoint` 콜백                                      |
|-----------------------|--------------------------------------------------------------|-------------------------------------------------------------|
| **목적**              | 과적합 방지를 위해 학습 조기 종료                            | 성능이 가장 좋은 모델을 저장                                |
| **기준 지표**         | 검증 손실(val_loss) 또는 정확도(val_accuracy) 등            | 지정한 모니터 지표 (기본은 val_loss)                        |
| **주요 기능**         | 성능 향상이 없으면 학습 멈춤                                 | 특정 조건 만족 시마다 모델을 저장                           |
| **파라미터 예시**     | `monitor='val_loss', patience=5`                             | `monitor='val_accuracy', save_best_only=True`              |
| **모델 저장 여부**    | 저장 기능 없음 (단지 학습 중단)                              | 자동으로 모델 파일(.h5 등) 저장                             |
| **사용 예시**         | `EarlyStopping(monitor='val_loss', patience=5)`              | `ModelCheckpoint(filepath='best.h5', save_best_only=True)` |
| **활용 목적**         | **무의미한 학습 반복 방지**                                 | **최적 모델을 저장하고 이후 불러오기 용이하게**            |

=> EarlyStopping 콜백을 ModelCheckpoint 콜백과 함께 사용하면 가장 낮은 검증 손실의 모델을 파일에 저장하고 검증 손실이 다시 상승할 때 훈련을 중지할 수 있고, 훈련을 중지한 다음 현재 모델의 파라미터를 최상의 파라미터로 되돌림

---
### 딥러닝 작동 구조 요약

```
[입력 데이터]
↓
[여러 개의 은닉층] ← 여기가 ‘Deep’한 부분
↓
[결과 출력]
```

**예시 상황: 고양이 vs 강아지 이미지 분류**

> "이 사진은 고양이일까? 강아지일까?"  
딥러닝은 이걸 사람이 직접 규칙을 안 짜도 스스로 판단할 수 있어요.

**딥러닝 작동 순서 (구체적으로!)**

**① 입력층 (Input Layer)**
- 데이터가 처음 들어오는 곳
- 예: 28×28 흑백 이미지 → 총 784개의 숫자(픽셀 값)  
  → `[0.1, 0.2, 0.0, ..., 0.9]`

**② 은닉층 (Hidden Layers)**
- 여러 개의 뉴런(노드)로 구성됨
- 이전 층의 출력값들과 **가중치**로 연결됨

**뉴런의 작동 방식**
```
모든 입력 × 가중치 → 더함 → 활성화 함수 → 출력
```

**실제 계산 예시:**

입력값: `[1.0, 2.0, 3.0]`  
가중치: `[0.2, -0.5, 0.1]`  
→ 계산: 1×0.2 + 2×-0.5 + 3×0.1 = 0.2 - 1.0 + 0.3 = -0.5

→ 이 값을 **활성화 함수**(ReLU, sigmoid 등)에 넣어 **비선형성**을 부여

> "내가 받은 신호가 충분히 크면 다음 층으로 넘기자!"

**③ 출력층 (Output Layer)**
- 마지막 결과를 출력하는 층
- 예: 고양이 vs 강아지 → `[0.9, 0.1]`  
  → 고양이일 확률 90%, 강아지 10%

**딥러닝 학습 과정: 순전파 + 역전파**

**1. 순전파 (Forward Propagation)**
- 입력 데이터를 통과시키며 예측값 계산

**2. 손실 계산 (Loss Function)**
- 예측값과 실제 정답의 차이를 수치화  
  예: 정답 `[1, 0]`, 예측 `[0.9, 0.1]` → 손실 작음  
       정답 `[1, 0]`, 예측 `[0.1, 0.9]` → 손실 큼

**3. 역전파 (Backpropagation)**
- 손실을 바탕으로 **오차를 출력 → 입력 방향으로 전달**
- 각 가중치를 **얼마나 조정할지 계산**

**옵티마이저**
- 역전파가 계산한 오차를 바탕으로  
  **가중치를 실제로 업데이트**해주는 역할

**전체 딥러닝 흐름 요약**

1. **데이터 입력**
2. **순전파**로 예측값 계산
3. **손실함수**로 오차 계산
4. **역전파**로 오차를 뒤로 보내며 가중치 수정 방향 계산
5. **옵티마이저**로 가중치 실제 조정
6. 이 과정을 수천~수만 번 반복
7. 점점 더 **정확한 예측** 가능!

---
### 주요 옵티마이저들

#### SGD (Stochastic Gradient Descent)
- 기본 중의 기본!
- 경사하강법의 가장 단순한 형태
- 학습률(learning rate)을 일정하게 설정해 사용

#### 모멘텀(Momentum)
SGD에 **관성**을 추가한 버전
- 경사하강 + 이전 방향을 기억해서 더 부드럽고 빠르게 움직임
- 튀거나 멈칫하지 않고 **탄력적으로 이동**

=> 공이 굴러가듯이 "가던 방향 + 경사"로 업데이트

#### 네스테로프 모멘텀 (Nesterov)
모멘텀보다 한 단계 더 똑똑함
- **예상 위치를 먼저 계산하고**, 그 위치에서 기울기를 측정
- 더 미리 보고 조절하는 느낌 (속도 + 정확성 모두 챙김)

#### AdaGrad (Adaptive Gradient)
- 각 파라미터마다 학습률을 **다르게 조절**해줌
- 자주 업데이트되는 파라미터는 학습률을 줄이고 잘 안 바뀌는 애는 더 크게 움직이게 함

#### RMSprop
AdaGrad의 단점을 보완!
- 오래된 정보는 잊고, **최근 기울기를 중심으로 학습률을 조절**
- 그래서 더 오래 안정적으로 학습할 수 있음

=> 이미지·음성 처리에 자주 쓰이는 옵티마이저

#### Adam (Adaptive Moment Estimation)
RMSprop + 모멘텀의 장점만 합친 **최고 인기 옵티마이저**
- 학습률도 조절하고, 방향성(모멘텀)도 고려
- 대부분의 상황에서 성능이 좋아서 기본값으로 가장 많이 사용