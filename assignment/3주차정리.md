# 3주차 - Chapter 05 트리 알고리즘

- [MLDL 스터디 3회차 실습 코드](https://colab.research.google.com/drive/1iW6iJD9VwdqgHwcnrn-40NpJsVIsH3Ho#scrollTo=Yw6YhImE9XfD)

## 관련 추가 내용 정리

### 불순도(Impurity)란?

한 노드(데이터 그룹) 안에 다양한 클래스가 섞여 있는 정도를 수치로 나타낸 것<br>
→ 한 클래스만 있다면 불순도는 0 (완전히 순수)<br>
→ 클래스가 균등하게 섞여 있다면 불순도는 최대

> 결정 트리는 데이터를 분할하면서 **불순도를 최소화하는 방향으로 트리를 성장시킴**<br>
즉, **가장 순수한(한쪽 클래스로 치우친) 노드**가 되도록 조건을 선택함

### 불순도 척도

**지니 불순도**

무작위로 샘플을 하나 뽑아 다른 클래스로 잘못 분류될 확률

$$\text{Gini} = 1 - \sum_{i=1}^{k} p_i^2$$
- $p_i$: i번째 클래스의 비율

- **값 범위**: 0 (완전 순수) ~ 0.5 (이진 분류에서 최대 혼합)

> 클래스가 100% 한 쪽만 있다면 → `Gini = 0`<br>
> 클래스가 50%/50%로 섞이면 → `Gini = 0.5`

---
**엔트로피**

정보 이론에서 온 개념<br>
→ 정보가 많다는 건 예측이 어렵다는 것 = 불확실성 증가 = 불순도 증가

$$\text{Entropy} = - \sum_{i=1}^{k} p_i \log_2 p_i$$
- $p_i$: i번째 클래스의 비율

- **값 범위**: 0 (순수) ~ 1 (불확실성 최대)

> 클래스가 100% 한 쪽이면 → `Entropy = 0`<br>
> 클래스가 50%/50%로 섞이면 → `Entropy = 1`

---
**분산 (Variance) → 회귀용**

예측해야 하는 연속형 값들이 평균에서 얼마나 흩어져 있는지 측정<br>
→ 노드 안 값들이 비슷할수록 더 순수함

$$\text{Variance} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \bar{y})^2$$

---
**시각적 비교**

| 클래스 분포 | Gini | Entropy |
|--------------|------|---------|
| [1.0, 0.0]    | 0.0  | 0.0     |
| [0.8, 0.2]    | 0.32 | 0.72    |
| [0.5, 0.5]    | 0.5  | 1.0     |

- 둘 다 비슷하지만 **엔트로피는 불순도 변화에 더 민감하게 반응**
- **Gini는 계산이 빠르기 때문에 실무에서 더 많이 사용됨**

---
**Gini vs Entropy 비교**

| 기준              | Gini                            | Entropy                        |
|-------------------|----------------------------------|--------------------------------|
| **해석**           | 잘못 분류될 확률                | 정보의 불확실성                |
| **계산 속도**      | 빠름                             | 느림                           |
| **분류 성능 차이** | 거의 없음                       | 거의 없음                      |
| **사용 예**        | CART 알고리즘 (기본값)          | ID3, C4.5 알고리즘             |

- 두 척도 모두 “**어떻게 하면 데이터를 더 순수하게 나눌 수 있을까?**”를 평가하는 도구
- 일반적으로 **Gini**가 더 많이 사용되며, **Entropy**는 더 민감한 분할이 필요할 때 사용됨

### 정보 차이(Information Gain)란?

노드를 특정 속성(피처)으로 나눴을 때, 그로 인해 불확실성이 얼마나 감소했는지를 나타내는 값<br>
즉, **정보 차이 = 나누기 전의 불확실성 - 나누고 난 후의 평균 불확실성**

- **분기 전**: 한 노드에 여러 클래스가 섞여 있음 → **불확실성 ↑**
- **특정 조건으로 분기**했더니:
  - 한쪽은 거의 A 클래스
  - 한쪽은 거의 B 클래스<br>
  → **불확실성 ↓**

이렇게 **불확실성이 줄어들었다면 → 정보 차이 증가**

즉, **정보 차이가 클수록 그 피처(속성)는 데이터를 잘 나누는 기준**이 된다는 뜻임

<br>

### 부스팅(Boosting)이란?

여러 개의 성능 낮은 모델(약한 모델)을 순차적으로 학습시키면서, 성능이 뛰어난 하나의 모델로 만드는 기법

**대표적인 부스팅 알고리즘**
| 알고리즘        | 주요 특징 및 설명 |
|----------------|------------------|
| **AdaBoost**   | - 최초의 부스팅 알고리즘<br>- 틀린 샘플에 가중치를 높여 다음 모델이 보완<br>- 단순하지만 노이즈에 민감<br>- 주로 얕은 트리 사용 |
| **Gradient Boosting (GBM)** | - 잔차(오차)를 보완하는 방식으로 트리 순차 학습<br>- 손실 함수의 기울기를 따라 최적화<br>- 성능 좋지만 느리고 병렬 처리 불가 |
| **XGBoost**    | - Gradient Boosting을 최적화<br>- L1/L2 정규화로 과적합 방지<br>- 병렬 학습, 조기 종료, 결측값 자동 처리<br>- 중소규모 데이터에 강함 |
| **LightGBM**   | - Microsoft 개발<br>- Leaf-wise 성장 방식으로 성능 극대화<br>- 히스토그램 기반 처리로 빠르고 메모리 효율적<br>- 대용량, 고차원 데이터에 적합<br>- 과적합 방지를 위한 튜닝 중요 |
| **CatBoost**   | - Yandex 개발, "Category + Boosting"<br>- 범주형 변수 자동 인식 및 처리에 특화<br>- 순서 인코딩 방식으로 데이터 누수 방지<br>- 튜닝 없이도 좋은 성능, 사용 쉬움 |

### 주요 매개변수 설명

**`XGBClassifier`**

- `max_depth`: 트리의 최대 깊이
  - 과적합 방지를 위한 핵심 파라미터
  - 너무 깊으면 학습 데이터에 과적합될 수 있음
  - 일반적으로 3~10 사이 값이 적절

- `learning_rate`: 학습률 (η)
  - 각 트리의 기여도를 조절
  - **작을수록** 학습 속도는 느리지만 **성능이 안정적**
  - 보통 0.01~0.3 사이 값 사용

- `n_estimators`: 트리 개수
  - 부스팅 라운드 수
  - `early_stopping_rounds`와 함께 조기 종료 가능

- `subsample`: 데이터 샘플링 비율
  - 각 트리를 학습할 때 사용하는 샘플 비율 (0~1)
  - 과적합 방지 목적, 보통 0.5~1.0 사용

- `colsample_bytree`: 특성 샘플링 비율
  - 각 트리 생성 시 사용할 feature 비율
  - 고차원 데이터에서 유용

- `reg_alpha`: L1 정규화 항 (α)
  - 불필요한 특성 제거에 도움
  - 값이 클수록 많은 특성이 0으로 설정됨

- `reg_lambda`: L2 정규화 항 (λ)
  - 가중치 크기를 억제해 모델 안정성 향상

- `gamma`: 가지 분할 최소 손실 감소 값
  - 클수록 분할을 덜 수행하여 단순한 트리 생성
  - 과적합 방지에 유리

- `objective`: 손실 함수 설정
  - `'binary:logistic'`: 이진 분류
  - `'multi:softmax'`: 다중 클래스 분류 (추가로 `num_class` 필요)
  - `'reg:squarederror'`: 회귀 문제

---
**`LGBMClassifier`**

- `num_leaves`: 트리의 리프 노드 수
  - 모델 복잡도를 조절하는 핵심 파라미터
  - 너무 크면 과적합 발생 가능

- `max_depth`: 트리 최대 깊이
  - `num_leaves`와 함께 과적합 방지
  - 일반적으로 5~15 사이 사용

- `learning_rate`: 학습률
  - 작을수록 학습이 느리지만 더 일반화된 모델 생성
  - 0.01~0.3 권장

- `n_estimators`: 부스팅 라운드 수
  - 생성할 트리 개수
  - `early_stopping_rounds`와 함께 사용 가능

- `subsample`: 데이터 샘플링 비율
  - 과적합 방지 목적
  - 보통 0.6~1.0 사용

- `feature_fraction` (`colsample_bytree`): 특성 샘플링 비율
  - 각 트리마다 사용할 feature 비율
  - 고차원 데이터에 효과적

- `reg_alpha`: L1 정규화 항
  - 모델 복잡도 감소 및 불필요한 특성 제거

- `reg_lambda`: L2 정규화 항
  - 과적합 방지를 위해 가중치 크기 제어

- `min_child_samples`: 리프 노드 최소 데이터 수
  - 최소 노드 크기 설정으로 과적합 방지

- `boosting_type`: 부스팅 방식
  - `'gbdt'`: 기본값, Gradient Boosting
  - `'dart'`: Dropout 기반, 과적합 방지
  - `'goss'`: One-Side Sampling으로 학습 속도 향상

- `objective`: 손실 함수
  - `'binary'`: 이진 분류
  - `'multiclass'`: 다중 클래스 분류
  - `'regression'`: 회귀 문제