# 5주차 - Chapter 08 합성곱 신경망의 구성 요소/합성곱 신경망을 사용한 이미지 분류

- [MLDL 스터디 5회차 실습 코드](https://colab.research.google.com/drive/1fgsDtqpgiVrID70hN8BRXDAiBfqOFN7J)

## 관련 추가 내용 정리

### 합성곱을 하는 이유: 공간적 패턴을 감지하려고
이미지는 단순히 픽셀 값의 모음이 아니라,
**공간 구조**가 중요해요.
- 예: 고양이의 눈, 귀, 수염은 특정 모양과 위치에 나타남
- 합성곱 필터는 **작은 창으로 이미지를 훑으면서**
그 지역(local)에 있는 **모양**을 감지해요

=> 완전 연결 신경망처럼 전체를 한 번에 보는 게 아니라
**부분을 관찰하면서 특징을 찾아내는 구조**가 이미지에 훨씬 잘 맞음

---
### 패딩을 하는 이유: 가장자리 정보도 놓치지 않으려고
패딩을 안 하면, 합성곱을 거치면서
**이미지의 가장자리는 덜 처리되고 손실**돼요.
- 예: 얼굴 사진에서 귀, 턱처럼 가장자리에 있는 정보가 사라질 수 있음
- 그래서 **same 패딩**으로 이미지 크기를 유지하면서
**가장자리도 동등하게 다룰 수 있게** 만들어요.

=> 경계에 있는 특징도 공정하게 분석하기 위한 설계

---
### 풀링을 하는 이유: 크기 줄이고 요약하려고
CNN은 중간에 생성되는 특성 맵이 매우 많고 큽니다.
이를 그냥 쌓으면 메모리·연산량이 폭증하죠.
- **풀링은 정보를 요약**하면서 크기를 줄여줘요.
- 예: (2,2) 풀링은 주변 4픽셀 중 가장 강한 신호 1개만 남김

=> 핵심 특징만 남기고, 덜 중요한 정보는 줄이는 "정보 압축" 단계

---
### 계층적으로 깊어지는 이유: 단순한 특징 → 복잡한 개념으로 확장하려고
- 앞쪽 필터는 직선, 모서리 같은 단순한 특징을 찾고
- 뒤로 갈수록 눈, 입, 고양이 얼굴 같은 복잡한 구조를 인식해요

이게 가능하려면 중간 단계에서 계속 **특성 추출 → 요약 → 추출**이 반복돼야 하므로 **CNN은 계층적으로 깊어지는 구조**를 갖습니다.

=> 추상화 단계를 거치면서 점점 더 "의미 있는 정보"로 나아가는 구조